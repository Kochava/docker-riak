## Where to emit the default log messages (typically at 'info'
## severity):
## off: disabled
## file: the file specified by log.console.file
## console: to standard output (seen when using `riak attach-direct`)
## both: log.console.file and standard out.
##
## Default: file
##
## Acceptable values:
##   - one of: off, file, console, both
log.console = console

## The severity level of the console log, default is 'info'.
##
## Default: info
##
## Acceptable values:
##   - one of: debug, info, notice, warning, error, critical, alert, emergency, none
log.console.level = info

## When 'log.console' is set to 'file' or 'both', the file where
## console messages will be logged.
##
## Default: $(platform_log_dir)/console.log
##
## Acceptable values:
##   - the path to a file
log.console.file = $(platform_log_dir)/console.log

## The file where error messages will be logged.
##
## Default: $(platform_log_dir)/error.log
##
## Acceptable values:
##   - the path to a file
log.error.file = $(platform_log_dir)/error.log

## When set to 'on', enables log output to syslog.
##
## Default: off
##
## Acceptable values:
##   - on or off
log.syslog = off

## Whether to enable the crash log.
##
## Default: on
##
## Acceptable values:
##   - on or off
log.crash = on

## If the crash log is enabled, the file where its messages will
## be written.
##
## Default: $(platform_log_dir)/crash.log
##
## Acceptable values:
##   - the path to a file
log.crash.file = $(platform_log_dir)/crash.log

## Maximum size in bytes of individual messages in the crash log
##
## Default: 64KB
##
## Acceptable values:
##   - a byte size with units, e.g. 10GB
log.crash.maximum_message_size = 64KB

## Maximum size of the crash log in bytes, before it is rotated
##
## Default: 10MB
##
## Acceptable values:
##   - a byte size with units, e.g. 10GB
log.crash.size = 10MB

## The schedule on which to rotate the crash log.  For more
## information see:
## https://github.com/basho/lager/blob/master/README.md#internal-log-rotation
##
## Default: $D0
##
## Acceptable values:
##   - text
log.crash.rotation = $D0

## The number of rotated crash logs to keep. When set to
## 'current', only the current open log file is kept.
##
## Default: 5
##
## Acceptable values:
##   - an integer
##   - the text "current"
log.crash.rotation.keep = 5

## Name of the Erlang node
##
## Default: riak@127.0.0.1
##
## Acceptable values:
##   - text
nodename = riak@127.0.0.1

## Cookie for distributed node communication.  All nodes in the
## same cluster should use the same cookie or they will not be able to
## communicate.
##
## Default: riak
##
## Acceptable values:
##   - text
distributed_cookie = riak

## Sets the number of threads in async thread pool, valid range
## is 0-1024. If thread support is available, the default is 64.
## More information at: http://erlang.org/doc/man/erl.html
##
## Default: 64
##
## Acceptable values:
##   - an integer
erlang.async_threads = 64

## The number of concurrent ports/sockets
## Valid range is 1024-134217727
##
## Default: 262144
##
## Acceptable values:
##   - an integer
erlang.max_ports = 262144

## Set scheduler forced wakeup interval. All run queues will be
## scanned each Interval milliseconds. While there are sleeping
## schedulers in the system, one scheduler will be woken for each
## non-empty run queue found. An Interval of zero disables this
## feature, which also is the default.
## This feature is a workaround for lengthy executing native code, and
## native code that do not bump reductions properly.
## More information: http://www.erlang.org/doc/man/erl.html#+sfwi
##
## Default: 500
##
## Acceptable values:
##   - an integer
## erlang.schedulers.force_wakeup_interval = 500

## Enable or disable scheduler compaction of load. By default
## scheduler compaction of load is enabled. When enabled, load
## balancing will strive for a load distribution which causes as many
## scheduler threads as possible to be fully loaded (i.e., not run out
## of work). This is accomplished by migrating load (e.g. runnable
## processes) into a smaller set of schedulers when schedulers
## frequently run out of work. When disabled, the frequency with which
## schedulers run out of work will not be taken into account by the
## load balancing logic.
## More information: http://www.erlang.org/doc/man/erl.html#+scl
##
## Default: false
##
## Acceptable values:
##   - one of: true, false
## erlang.schedulers.compaction_of_load = false

## Enable or disable scheduler utilization balancing of load. By
## default scheduler utilization balancing is disabled and instead
## scheduler compaction of load is enabled which will strive for a
## load distribution which causes as many scheduler threads as
## possible to be fully loaded (i.e., not run out of work). When
## scheduler utilization balancing is enabled the system will instead
## try to balance scheduler utilization between schedulers. That is,
## strive for equal scheduler utilization on all schedulers.
## More information: http://www.erlang.org/doc/man/erl.html#+sub
##
## Acceptable values:
##   - one of: true, false
## erlang.schedulers.utilization_balancing = true

## Number of partitions in the cluster (only valid when first
## creating the cluster). Must be a power of 2, minimum 8 and maximum
## 1024.
##
## Default: 64
##
## Acceptable values:
##   - an integer
## ring_size = 64

## Number of concurrent node-to-node transfers allowed.
##
## Default: 2
##
## Acceptable values:
##   - an integer
## transfer_limit = 2

## Default cert location for https can be overridden
## with the ssl config variable, for example:
##
## Acceptable values:
##   - the path to a file
## ssl.certfile = $(platform_etc_dir)/cert.pem

## Default key location for https can be overridden with the ssl
## config variable, for example:
##
## Acceptable values:
##   - the path to a file
## ssl.keyfile = $(platform_etc_dir)/key.pem

## Default signing authority location for https can be overridden
## with the ssl config variable, for example:
##
## Acceptable values:
##   - the path to a file
## ssl.cacertfile = $(platform_etc_dir)/cacertfile.pem

## DTrace support Do not enable 'dtrace' unless your Erlang/OTP
## runtime is compiled to support DTrace.  DTrace is available in
## R15B01 (supported by the Erlang/OTP official source package) and in
## R14B04 via a custom source repository & branch.
##
## Default: off
##
## Acceptable values:
##   - on or off
dtrace = off

## Platform-specific installation paths (substituted by rebar)
##
## Default: ./bin
##
## Acceptable values:
##   - the path to a directory
platform_bin_dir = /usr/local/riak/bin

##
## Default: ./data
##
## Acceptable values:
##   - the path to a directory
platform_data_dir = /var/lib/riak

##
## Default: ./etc
##
## Acceptable values:
##   - the path to a directory
platform_etc_dir = ./etc

##
## Default: ./lib
##
## Acceptable values:
##   - the path to a directory
platform_lib_dir = /usr/local/riak/lib

##
## Default: ./log
##
## Acceptable values:
##   - the path to a directory
platform_log_dir = /var/log

## Enable consensus subsystem. Set to 'on' to enable the
## consensus subsystem used for strongly consistent Riak operations.
##
## Default: off
##
## Acceptable values:
##   - on or off
## strong_consistency = on

## listener.http.<name> is an IP address and TCP port that the Riak
## HTTP interface will bind.
##
## Default: 127.0.0.1:8098
##
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:10011
listener.http.internal = 0.0.0.0:8098

## listener.protobuf.<name> is an IP address and TCP port that the Riak
## Protocol Buffers interface will bind.
##
## Default: 127.0.0.1:8087
##
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:10011
listener.protobuf.internal = 0.0.0.0:8087

## The maximum length to which the queue of pending connections
## may grow. If set, it must be an integer > 0. If you anticipate a
## huge number of connections being initialized *simultaneously*, set
## this number higher.
##
## Default: 128
##
## Acceptable values:
##   - an integer
## protobuf.backlog = 128

## listener.https.<name> is an IP address and TCP port that the Riak
## HTTPS interface will bind.
##
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:10011
## listener.https.internal = 127.0.0.1:8098

## How Riak will repair out-of-sync keys. Some features require
## this to be set to 'active', including search.
## * active: out-of-sync keys will be repaired in the background
## * passive: out-of-sync keys are only repaired on read
## * active-debug: like active, but outputs verbose debugging
## information
##
## Default: active
##
## Acceptable values:
##   - one of: active, passive, active-debug
anti_entropy = active

##
## Default: passive
##
## Acceptable values:
##   - one of: active, passive
tictacaae_active = passive

## A path under which aae data files will be stored.
##
## Default: $(platform_data_dir)/tictac_aae
##
## Acceptable values:
##   - the path to a directory
tictacaae_dataroot = $(platform_data_dir)/tictac_aae

## Parallel key store type
## When running in parallel mode, which will be the default if the backend does
## not support native tictac aae (i.e. is not leveled), what type of parallel
## key store should be kept - leveled_ko (leveled and key-ordered), or
## leveled_so (leveled and segment ordered).
## When running in native mode, this setting is ignored
##
## Default: leveled_ko
##
## Acceptable values:
##   - one of: leveled_ko, leveled_so
## tictacaae_parallelstore = leveled_ko

## Minimum Rebuild Wait
## The minimum number of hours to wait between rebuilds.  Default value is 2
## weeks
##
## Default: 336
##
## Acceptable values:
##   - an integer
tictacaae_rebuildwait = 336

## Maximum Rebuild Delay
## The number of seconds which represents the length of the period in which the
## next rebuild will be scheduled.  So if all vnodes are scheduled to rebuild
## at the same time, they will actually rebuild randomly between 0 an this
## value (in seconds) after the rebuild time. Default value is 4 days
##
## Default: 345600
##
## Acceptable values:
##   - an integer
tictacaae_rebuilddelay = 345600

## Store heads in parallel key stores
## If running a parallel key store, the whole "head" object may be stored to
## allow for fold_heads queries to be run against the parallel store.
## Alternatively, the cost of the paralle key store can be reduced by storing
## only a minimal data set necessary for AAE and monitoring
##
## Default: disabled
##
## Acceptable values:
##   - enabled or disabled
## tictacaae_storeheads = disabled

## Pool Strategy - should a single node_worker_pool or multiple pools be
## used for queueing potentially longer-running "background" queries
##
## Default: dscp
##
## Acceptable values:
##   - one of: none, single, dscp
## worker_pool_strategy = dscp

## Pool Sizes - sizes for individual node_worker_pools
## Only relevant if single or dscp strategy chosen.  Set
## `node_worker_pool_size` if a `single` pool strategy is being used, or set
## `af_worker_pool_size` and `be_worker_pool_size` if a multiple pool strategy
## has been chosen.
## Separate assured forwarding pools will be used of `af_worker_pool_size` for
## informational aae_folds (find_keys, object_stats) and functional folds
## (merge_tree_range, fetch_clock_range).  The be_pool is used only for tictac
## AAE rebuilds at present
##
## Default: 2
##
## Acceptable values:
##   - an integer
node_worker_pool_size = 2

##
## Default: 1
##
## Acceptable values:
##   - an integer
af1_worker_pool_size = 1

##
## Default: 1
##
## Acceptable values:
##   - an integer
af2_worker_pool_size = 1

##
## Default: 2
##
## Acceptable values:
##   - an integer
af3_worker_pool_size = 2

##
## Default: 1
##
## Acceptable values:
##   - an integer
af4_worker_pool_size = 1

##
## Default: 1
##
## Acceptable values:
##   - an integer
be_worker_pool_size = 1

## Backend PUT Pause (ms).
## If the backend PUT has resulted in a pause request, then how long should
## the vnode pause for?  This is measured in ms, and currently only applies
## to the leveled backend
##
## Default: 10
##
## Acceptable values:
##   - an integer
## backend_pause_ms = 10

## Whether to allow node to participate in coverage queries.
## This is used as a manual switch to stop nodes in incomplete states
## (E.g. doing a full partition repair, or node replace) from participating
## in coverage queries, as their information may be incomplete (e.g. 2i
## issues seen in these circumstances).
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## participate_in_coverage = enabled

## Specifies the storage engine used for Riak's key-value data
## and secondary indexes (if supported).
##
## Default: bitcask
##
## Acceptable values:
##   - one of: bitcask, leveldb, leveled, memory, multi, prefix_multi
storage_backend = bitcask

## Simplify prefix_multi configuration for Riak CS. Keep this
## commented out unless Riak is configured for Riak CS.
##
## Acceptable values:
##   - an integer
## cs_version = 20000

## Controls which binary representation of a riak value is stored
## on disk.
## * 0: Original erlang:term_to_binary format. Higher space overhead.
## * 1: New format for more compact storage of small values.
## If using the leveled backend object_format 1 will always be used, when
## persisting data into the backend - even if 0 has been configured here
##
## Default: 1
##
## Acceptable values:
##   - the integer 1
##   - the integer 0
object.format = 1

## Reading or writing objects bigger than this size will write a
## warning in the logs.
##
## Default: 5MB
##
## Acceptable values:
##   - a byte size with units, e.g. 10GB
object.size.warning_threshold = 5MB

## Writing an object bigger than this will send a failure to the
## client.
##
## Default: 50MB
##
## Acceptable values:
##   - a byte size with units, e.g. 10GB
object.size.maximum = 50MB

## Writing an object with more than this number of siblings will
## generate a warning in the logs.
##
## Default: 25
##
## Acceptable values:
##   - an integer
object.siblings.warning_threshold = 25

## Writing an object with more than this number of siblings will
## send a failure to the client.
##
## Default: 100
##
## Acceptable values:
##   - an integer
object.siblings.maximum = 100

## Whether to allow list buckets.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.list_buckets = enabled

## Whether to allow streaming list buckets.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.stream_list_buckets = enabled

## Whether to allow list keys.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.list_keys = enabled

## Whether to allow streaming list keys.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.stream_list_keys = enabled

## Whether to allow secondary index queries.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.secondary_index = enabled

## Whether to allow streaming secondary index queries.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.stream_secondary_index = enabled

## Whether to allow term-based map-reduce.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.map_reduce = enabled

## Whether to allow JavaScript map-reduce.
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.riak_kv.map_reduce_js = enabled

## A path under which bitcask data files will be stored.
##
## Default: $(platform_data_dir)/bitcask
##
## Acceptable values:
##   - the path to a directory
bitcask.data_root = $(platform_data_dir)/bitcask

## Configure how Bitcask writes data to disk.
## erlang: Erlang's built-in file API
## nif: Direct calls to the POSIX C API
## The NIF mode provides higher throughput for certain
## workloads, but has the potential to negatively impact
## the Erlang VM, leading to higher worst-case latencies
## and possible throughput collapse.
##
## Default: erlang
##
## Acceptable values:
##   - one of: erlang, nif
bitcask.io_mode = erlang

## Set to 'off' to disable the admin panel.
##
## Default: off
##
## Acceptable values:
##   - on or off
riak_control = off

## Authentication mode used for access to the admin panel.
##
## Default: off
##
## Acceptable values:
##   - one of: off, userlist
riak_control.auth.mode = off

## If riak control's authentication mode (riak_control.auth.mode)
## is set to 'userlist' then this is the list of usernames and
## passwords for access to the admin panel.
## To create users with given names, add entries of the format:
## riak_control.auth.user.USERNAME.password = PASSWORD
## replacing USERNAME with the desired username and PASSWORD with the
## desired password for that user.
##
## Acceptable values:
##   - text
## riak_control.auth.user.admin.password = pass

## This parameter defines the percentage of total server memory
## to assign to LevelDB. LevelDB will dynamically adjust its internal
## cache sizes to stay within this size.  The memory size can
## alternately be assigned as a byte count via leveldb.maximum_memory
## instead.
##
## Default: 70
##
## Acceptable values:
##   - an integer
leveldb.maximum_memory.percent = 70

## Enables or disables the compression of data on disk.
## Enabling (default) saves disk space.  Disabling may reduce read
## latency but increase overall disk activity.  Option can be
## changed at any time, but will not impact data on disk until
## next time a file requires compaction.
##
## Default: on
##
## Acceptable values:
##   - on or off
leveldb.compression = on

## Selection of compression algorithms.  snappy is
## original compression supplied for leveldb.  lz4 is new
## algorithm that compresses to similar volume but averages twice
## as fast on writes and four times as fast on reads.
##
## Acceptable values:
##   - one of: snappy, lz4
leveldb.compression.algorithm = lz4

##
## Default: on
##
## Acceptable values:
##   - on or off
## multi_backend.name.leveldb.compression = on

##
## Acceptable values:
##   - one of: snappy, lz4
## multi_backend.name.leveldb.compression.algorithm = lz4

## A path under which leveled data files will be stored.
##
## Default: $(platform_data_dir)/leveled
##
## Acceptable values:
##   - the path to a directory
leveled.data_root = $(platform_data_dir)/leveled

## Strategy for flushing data to disk
## Can be set to riak_sync, sync (if OTP > 16) or none.  Use none, and the OS
## will flush when most efficient.  Use riak_sync or sync to flush after every
## PUT (not recommended wihtout some hardware support e.g. flash drives and/or
## Flash-backed Write Caches)
##
## Default: none
##
## Acceptable values:
##   - text
leveled.sync_strategy = none

## Compression method
## Can be lz4 or native (which will use the Erlang native zlib compression)
## within term_to_binary
##
## Default: native
##
## Acceptable values:
##   - text
leveled.compression_method = native

## Compression point
## The point at which compression is applied to the Journal (the Ledger is
## always compressed).  Use on_receipt or on_compact.  on_compact is suitable
## when values are unlikely to yield much benefit from compression
## (compression is only attempted when compacting)
##
## Default: on_receipt
##
## Acceptable values:
##   - text
leveled.compression_point = on_receipt

## Log level
## Can be debug, info, warn, error or critical
## Set the minimum log level to be used within leveled.  Leveled will log many
## lines to allow for stats to be etracted by those using log indexers such as
## Splunk
##
## Default: info
##
## Acceptable values:
##   - text
leveled.log_level = info

## The approximate size (in bytes) when a Journal file should be rolled.
## Normally keep this as around the size of o(100K) objects.  Default is 1GB.
## Note that on startup an actual maximum size will be chosen which varies by
## a random factor from this point - to avoid coordination of roll events
## across vnodes.
##
## Default: 1000000000
##
## Acceptable values:
##   - an integer
leveled.journal_size = 1000000000

## The approximate count of objects when a Journal file should be rolled.
## This time measured in object count, a file will be rolled if either the
## object count or the journal size limit is reached.  Default 200K.
## Note that on startup an actual maximum size will be chosen which varies by
## a random factor from this point - to avoid coordination of roll events
## across vnodes.
##
## Default: 200000
##
## Acceptable values:
##   - an integer
leveled.journal_objectcount = 200000

## The level of the ledger to be pre-loaded into the page cache
## Depending on how much memory is available for the page cache, and how much
## disk I/O activity can be tolerated at startup - then the level at which the
## ledger is forced into the page cache can be controlled by configuration.
##
## Default: 4
##
## Acceptable values:
##   - an integer
leveled.ledger_pagecachelevel = 4

## The number of journal compactions per vnode per day
## The higher the value, the more compaction runs, and the sooner space is
## recovered.  But each run has a cost
##
## Default: 24
##
## Acceptable values:
##   - an integer
leveled.compaction_runs_perday = 24

## Compaction Low Hour
## The hour of the day in which journal compaction can start.  Use Low hour
## of 0 and High hour of 23 to have no compaction window (i.e. always compact
## regardless of time of day)
##
## Default: 0
##
## Acceptable values:
##   - an integer
leveled.compaction_low_hour = 0

## Compaction Top Hour
## The hour of the day, after which journal compaction should stop.
## If low hour > top hour then, compaction will work overnight between low
## hour and top hour (inclusive).  Timings rely on server's view of local time
##
## Default: 23
##
## Acceptable values:
##   - an integer
leveled.compaction_top_hour = 23

## Max Journal Files Per Compaction Run
## In a single compaction run, what is the maximum number of consecutive files
## which may be compacted.
##
## Default: 4
##
## Acceptable values:
##   - an integer
leveled.max_run_length = 4

## The approximate size (in bytes) when a Journal file should be rolled.
## Normally keep this as around the size of o(100K) objects.  Default is 1GB.
## Note that on startup an actual maximum size will be chosen which varies by
## a random factor from this point - to avoid coordination of roll events
## across vnodes.
##
## Default: 1000000000
##
## Acceptable values:
##   - an integer
multi_backend.name.leveled.journal_size = 1000000000

## The approximate count of objects when a Journal file should be rolled.
## This time measured in object count, a file will be rolled if either the
## object count or the journal size limit is reached.  Default 200K.
## Note that on startup an actual maximum size will be chosen which varies by
## a random factor from this point - to avoid coordination of roll events
## across vnodes.
##
## Default: 200000
##
## Acceptable values:
##   - an integer
multi_backend.name.leveled.journal_objectcount = 200000

## The level of the ledger to be pre-loaded into the page cache
## Depending on how much memory is available for the page cache, and how much
## disk I/O activity can be tolerated at startup - then the level at which the
## ledger is forced into the page cache can be controlled by configuration.
##
## Default: 4
##
## Acceptable values:
##   - an integer
multi_backend.name.leveled.ledger_pagecachelevel = 4

## To enable Search set this 'on'.
##
## Default: off
##
## Acceptable values:
##   - on or off
search = off

## How long Riak will wait for Solr to start. The start sequence
## will be tried twice. If both attempts timeout, then the Riak node
## will be shutdown. This may need to be increased as more data is
## indexed and Solr takes longer to start. Values lower than 1s will
## be rounded up to the minimum 1s.
##
## Default: 30s
##
## Acceptable values:
##   - a time duration with units, e.g. '10s' for 10 seconds
search.solr.start_timeout = 30s

## The port number which Solr binds to.
## NOTE: Binds on every interface.
##
## Default: 8093
##
## Acceptable values:
##   - an integer
search.solr.port = 8093

## The port number which Solr JMX binds to.
## NOTE: Binds on every interface.
##
## Default: 8985
##
## Acceptable values:
##   - an integer
search.solr.jmx_port = 8985

## The options to pass to the Solr JVM.  Non-standard options,
## i.e. -XX, may not be portable across JVM implementations.
## E.g. -XX:+UseCompressedStrings
##
## Default: -d64 -Xms1g -Xmx1g -XX:+UseCompressedOops
##
## Acceptable values:
##   - text
search.solr.jvm_options = -d64 -Xms1g -Xmx1g -XX:+UseCompressedOops

## The minimum batch size, in number of Riak objects. Any batches that
## are smaller than this amount will not be immediately flushed to Solr,
## but are guaranteed to be flushed within the
## "search.queue.batch.flush_interval".
##
## Default: 10
##
## Acceptable values:
##   - an integer
## search.queue.batch.minimum = 10

## The maximum batch size, in number of Riak objects. Any batches that are
## larger than this amount will be split, where the first
## search.queue.batch.maximum set of objects will be flushed to Solr, and
## the remaining objects enqueued for that index will be retained until the
## next batch is delivered. This parameter ensures that at most
## "search.queue.batch.maximum object" will be delivered into Solr in any
## given request.
##
## Default: 500
##
## Acceptable values:
##   - an integer
## search.queue.batch.maximum = 500

## The maximum delay between notification to flush batches to Solr. This
## setting is used to increase or decrease the frequency of batch delivery
## into Solr, specifically for relatively low-volume input into Riak. This
## setting ensures that data will be delivered into Solr in accordance with
## the "search.queue.batch.minimum" and "search.queue.batch.maximum"
## settings within the specified interval. Batches that are smaller than
## "search.queue.batch.minimum" will be delivered to Solr within this
## interval. This setting will generally hav no effect on heavily loaded
## systems.
##
## Default: 500ms
##
## Acceptable values:
##   - a time duration with units, e.g. '10s' for 10 seconds
##   - the text "infinity"
## search.queue.batch.flush_interval = 500ms

## The queue high watermark. If the total number of queued messages in a
## Solr Queue Worker instance exceeds this limit, then the calling vnode
## will be blocked until the total number falls below this limit. This
## parameter exercises flow control between Riak and the Riak
## Search batching subsystem if writes into Solr start to fall behind.
##
## Default: 1000
##
## Acceptable values:
##   - an integer
## search.queue.high_watermark = 1000

## The strategy for how we handle purging when we hit the
## search.queue.high_watermark.
## - purge_one -> Removes the oldest item on the queue from an
## erroring (references to fuses blown in the code) index in
## order to get below the search.queue.high_watermark.
## - purge_index -> Removes all items associated with one random
## erroring (references to fuses blown in the code) index in
## order to get below the search.queue.high_watermark.
## - off -> purging is disabled
##
## Default: purge_one
##
## Acceptable values:
##   - one of: purge_one, purge_index, off
## search.queue.high_watermark.purge_strategy = purge_one

## Whether to allow Yokozuna queries on this node
##
## Default: enabled
##
## Acceptable values:
##   - enabled or disabled
## cluster.job.yokozuna.query = enabled
